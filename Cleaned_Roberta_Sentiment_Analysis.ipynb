{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mw0CzZVFyEWh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaForSequenceClassification,get_linear_schedule_with_warmup\n",
    "# Import AdamW from torch.optim\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
    "from sklearn.metrics import accuracy_score,matthews_corrcoef\n",
    "\n",
    "from tqdm import tqdm, trange,tnrange,tqdm_notebook\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "# %matplotlib inline # Remove or comment out this line if not in a Jupyter Notebook\n",
    "# Instead, use plt.show() after creating your plots\n",
    "# For example:\n",
    "# plt.plot([1, 2, 3], [4, 5, 6])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g2J1HVbydV4"
   },
   "source": [
    "**ROBERTA - X-Twitter Sentiment Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT5xgauNy9J3"
   },
   "source": [
    "Identify and specify the GPU as the device, later in training loop we will load data into device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tj8OoK6y9so",
    "outputId": "105e8400-cb23-4845-a454-f387f1fe3ba3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaForSequenceClassification,get_linear_schedule_with_warmup\n",
    "# Import AdamW from torch.optim\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
    "from sklearn.metrics import accuracy_score,matthews_corrcoef\n",
    "\n",
    "from tqdm import tqdm, trange,tnrange,tqdm_notebook\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "# %matplotlib inline # Remove or comment out this line if not in a Jupyter Notebook\n",
    "# Instead, use plt.show() after creating your plots\n",
    "# For example:\n",
    "# plt.plot([1, 2, 3], [4, 5, 6])\n",
    "# plt.show()\n",
    "\n",
    "# identify and specify the GPU as the device, later in training loop we will load data into device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "# Check if CUDA is available before trying to get device name\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")\n",
    "\n",
    "SEED = 19\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == torch.device(\"cuda\"):\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oE286JNzLJI"
   },
   "source": [
    "**Read File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYYgl-iszJ3v"
   },
   "outputs": [],
   "source": [
    "# Assuming the CSV file is in the same directory as your script\n",
    "df_train = pd.read_csv(\"Twitter_Data.csv\")  # Remove the incorrect path\n",
    "\n",
    "# OR, if the file is in a different location, use the correct path\n",
    "# df_train = pd.read_csv(\"/path/to/your/file/Twitter_Data.csv\") # Replace with your correct path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "id": "YnzKQNPBzpPK",
    "outputId": "de64937e-0acc-4df6-a692-c29abe23f774"
   },
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "udKfpM5NztGf",
    "outputId": "e0b0b24d-4b73-4988-82c7-18a72a37bcbd"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzS3QLn6zuBn",
    "outputId": "ac215345-3258-47bf-d58f-bf3e2902572e"
   },
   "outputs": [],
   "source": [
    "df_train['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "oYU240oFzwaC",
    "outputId": "964760db-6285-4e40-c85c-9d415643efb8"
   },
   "outputs": [],
   "source": [
    "df_train['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBsuUF8czyxX"
   },
   "outputs": [],
   "source": [
    "df_train = df_train[~df_train['category'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHFfab6-z0vv"
   },
   "outputs": [],
   "source": [
    "df_train = df_train[~df_train['clean_text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iALhx1rUz2hC"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "df_train['category_1'] = labelencoder.fit_transform(df_train['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "ffqBcS22z4-f",
    "outputId": "ccbee99e-a687-4275-d375-df9bf4472308"
   },
   "outputs": [],
   "source": [
    "df_train[['category','category_1']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bj-xD6erz65f"
   },
   "outputs": [],
   "source": [
    "df_train.rename(columns={'category_1':'label'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fs1YAxryz8lh",
    "outputId": "b5318bb9-f850-4c0a-b87d-f9c6de885fe4"
   },
   "outputs": [],
   "source": [
    "## create label and sentence list\n",
    "sentences = df_train.clean_text.values\n",
    "\n",
    "#check distribution of data based on labels\n",
    "print(\"Distribution of data based on labels: \",df_train.label.value_counts())\n",
    "\n",
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway.\n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 256\n",
    "\n",
    "## Import ROBERTA tokenizer, that is used to convert our text into tokens that corresponds to ROBERTA library\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base',do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0hwqmQy0ADX",
    "outputId": "7696ffdf-2c4c-481d-f748-38807a1294bc"
   },
   "outputs": [],
   "source": [
    "input_ids = [tokenizer.encode(sent, add_special_tokens=True,\n",
    "                              max_length=MAX_LEN,\n",
    "                              pad_to_max_length=True,truncation=True) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfsEzV_60a12",
    "outputId": "51c847ac-a652-4ce0-a91b-1257a5767e90"
   },
   "outputs": [],
   "source": [
    "labels = df_train.label.values\n",
    "\n",
    "print(\"Actual sentence before tokenization: \",sentences[2])\n",
    "print(\"Encoded Input from dataset: \",input_ids[2])\n",
    "\n",
    "## Create attention mask\n",
    "attention_masks = []\n",
    "## Create a mask of 1 for all input tokens and 0 for all padding tokens\n",
    "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
    "print(attention_masks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_vxziXE0ch_"
   },
   "outputs": [],
   "source": [
    "train_inputs,validation_inputs,train_labels,validation_labels = train_test_split(input_ids,labels,random_state=41,test_size=0.1)\n",
    "train_masks,validation_masks,_,_ = train_test_split(attention_masks,input_ids,random_state=41,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUtiAX1o0hNX"
   },
   "outputs": [],
   "source": [
    "# convert all our data into torch tensors, required data type for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "train_data = TensorDataset(train_inputs,train_masks,train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1AUOgA6U0otG",
    "outputId": "45a96fdb-fdef-4c92-8679-9ce355ca9f2a"
   },
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9vTuuN70p34",
    "outputId": "d5438bd3-5107-4a09-bf88-04979633da46"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Load RoBERTa model with classification head\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3).to(device)\n",
    "\n",
    "# Training hyperparameters\n",
    "lr = 2e-5\n",
    "adam_epsilon = 1e-8\n",
    "epochs = 3\n",
    "\n",
    "# Calculate training steps\n",
    "num_training_steps = len(train_dataloader) * epochs\n",
    "num_warmup_steps = 0\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "# Reset gradients\n",
    "model.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCJscmIJ4c4W"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBc_CdP64hgu",
    "outputId": "89724adc-31ed-4a68-a4f6-b6e27335fef5"
   },
   "outputs": [],
   "source": [
    "# Function to print memory usage\n",
    "def print_memory_usage():\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Used: {mem.used / 1e9:.2f} GB / Total: {mem.total / 1e9:.2f} GB\")\n",
    "\n",
    "# Print memory before model loading\n",
    "print(\"Before model load:\")\n",
    "print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWWkeaBd4ltt",
    "outputId": "8b1e5673-ca2e-4e2e-c92f-979095ef177a"
   },
   "outputs": [],
   "source": [
    "# Load RoBERTa model on CPU\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3).to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iUyR5hTB4ou3",
    "outputId": "9f5c9f5d-d00a-4831-f2cb-14a80148a6ca"
   },
   "outputs": [],
   "source": [
    "# Print memory after model load\n",
    "print(\"After model load:\")\n",
    "print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dhmA7C84yeu"
   },
   "outputs": [],
   "source": [
    "# Set training hyperparameters\n",
    "lr = 2e-5\n",
    "adam_epsilon = 1e-8\n",
    "epochs = 3\n",
    "\n",
    "# Number of training steps\n",
    "num_training_steps = len(train_dataloader) * epochs  # Ensure train_dataloader is defined\n",
    "num_warmup_steps = 0\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "# Reset gradients\n",
    "model.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRWz2qpP-xbB",
    "outputId": "12183935-b74f-4c1f-fba0-52f816ec0ccf"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 38
    },
    "id": "vgReSBhI_Qa9",
    "outputId": "2837af6a-77c3-4e37-d9f7-f6bb48113325"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170,
     "referenced_widgets": [
      "c5b4154b0d8a4599a16b94a2159d9d0c",
      "e8c1f4392b884ac3b26897d3e425b7d5",
      "961a8d6ea601421aa8bbf21ee37b753f",
      "5d280e83c59841ad8453b5d867c61658",
      "ee1443fd87044572bdda8a10a86f6a57",
      "589cfa949e454d608ffc0964c2d487cb",
      "c91b7e6fc79145d7ad641f3e7a058392",
      "ccd6647d768444de8b79052694b706cc",
      "4ea6031d25254f66bf93ee9c4f5f81e8",
      "ceed5e94d987430da8abe046ad4d6106",
      "36e438d4d3d74ab38bc74e49ee881912",
      "add5e176d59c4140aca182886d22f242",
      "739f9009486c42dfb61b2d998bd3bb19",
      "e50b2534788c4a07875df4a86f3ceb8d",
      "af3abb8b00a84ed6bb30df155982b4fb",
      "29ae8544283944b39d5ae438b6591fdb",
      "75f5cff4ed1748919f8d05c142459b4a",
      "be77149fcaac4880854cd371f958c0cd",
      "ec8bbb832f164ea2bd319ca1752d80f8",
      "643349d441ff4ba09e7c040b0b880c79",
      "1786d28cea9e4e97bd570eaf21466370",
      "5522b80e344b4b40b3b53e30b9dabec2"
     ]
    },
    "id": "9dHVmBnz5ozV",
    "outputId": "d510ee41-07b9-40a3-dd6e-afc62b4444f2"
   },
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('distilroberta-base', num_labels=3).to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ebxvLUh7lfN"
   },
   "outputs": [],
   "source": [
    "## Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "learning_rate = []\n",
    "\n",
    "# Gradients gets accumulated by default\n",
    "model.zero_grad()\n",
    "\n",
    "# tnrange is a tqdm wrapper around the normal python range\n",
    "for _ in tnrange(1,epochs+1,desc='Epoch'):\n",
    "  print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n",
    "  # Calculate total loss for this epoch\n",
    "  batch_loss = 0\n",
    "\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    loss = outputs[0]\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip the norm of the gradients to 1.0\n",
    "    # Gradient clipping is not in AdamW anymore\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update learning rate schedule\n",
    "    scheduler.step()\n",
    "\n",
    "    # Clear the previous accumulated gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Update tracking variables\n",
    "    batch_loss += loss.item()\n",
    "\n",
    "  # Calculate the average loss over the training data.\n",
    "  avg_train_loss = batch_loss / len(train_dataloader)\n",
    "\n",
    "  #store the current learning rate\n",
    "  for param_group in optimizer.param_groups:\n",
    "    print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n",
    "    learning_rate.append(param_group['lr'])\n",
    "\n",
    "  train_loss_set.append(avg_train_loss)\n",
    "  print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n",
    "\n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables\n",
    "  eval_accuracy,eval_mcc_accuracy,nb_eval_steps = 0, 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[0].to('cpu').numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "\n",
    "    df_metrics=pd.DataFrame({'Epoch':epochs,'Actual_class':labels_flat,'Predicted_class':pred_flat})\n",
    "\n",
    "    tmp_eval_accuracy = accuracy_score(labels_flat,pred_flat)\n",
    "    tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n",
    "\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    eval_mcc_accuracy += tmp_eval_mcc_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(F'\\n\\tValidation Accuracy: {eval_accuracy/nb_eval_steps}')\n",
    "  print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKVyiFXh59jW"
   },
   "outputs": [],
   "source": [
    "## emotion labels\n",
    "label2int = {\n",
    "  \"Negative\": 0,\n",
    "  \"Neutral\": 1,\n",
    "  \"Positive\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8sw5FjA5_Gv"
   },
   "outputs": [],
   "source": [
    "print(classification_report(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values, target_names=label2int.keys(), digits=len(label2int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GYCQevO6BOH"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZM7yMZA6DI-"
   },
   "outputs": [],
   "source": [
    "confusion_matrix(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPEaYQa06EsA"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm=confusion_matrix(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values),\n",
    "                      classes=[0  ,1  ,2],\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
